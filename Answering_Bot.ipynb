{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answering Bot",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadidi/NLP_Chatbot/blob/main/Answering_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xknm1r215xEq"
      },
      "source": [
        "# **Answering Bot**\n",
        "\n",
        "*Developed by : Adam Kharsa*\n",
        "\n",
        "*Reviewed by : Souad Hadidi*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY7mmcj9pRcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72bd722c-9c3a-4c30-e58b-284cc5e98f85"
      },
      "source": [
        "# verify GPU availability\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxu0iAvYsHTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f07b651-b792-4982-8a1b-6bb15e5c3d75"
      },
      "source": [
        "# install huggingface libraries\n",
        "!pip install pytorch-pretrained-bert pytorch-nlp pytorch_transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.17.65)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.0.45)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.1.95)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.4.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.65 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.65)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.65->boto3->pytorch-pretrained-bert) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqd8Nwwnuh3u"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_transformers import BertTokenizer, BertConfig, BertModel\n",
        "from pytorch_transformers import AdamW, BertForQuestionAnswering\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9ANT1FssMH4"
      },
      "source": [
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkii9bEqsRDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92426638-82e2-4652-baa0-a4d7d3843642"
      },
      "source": [
        "#To run this code,\n",
        "#The attached file final_project needs to be on the root directory\n",
        "#of a google drive account\n",
        "#Once there, the weights and required training files will load and the\n",
        "#training time will just be loading the weights into the model\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DwYBV-2askA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ca3d2b-8f8d-4302-b0dd-7c3389159b2e"
      },
      "source": [
        "!ls /drive/My\\ Drive/final_project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cache_train\t  checkpoint-6000\t\t custom_predictions.json\n",
            "cache_validation  checkpoint-7000\t\t dev-v2.0.json\n",
            "checkpoint-1000   checkpoint-8000\t\t nbest_predictions.json\n",
            "checkpoint-10000  checkpoint-9000\t\t null_odds.json\n",
            "checkpoint-2000   checkpoint-final\t\t predictions.json\n",
            "checkpoint-3000   custom_input.json\t\t train-v2.0.json\n",
            "checkpoint-4000   custom_nbest_predictions.json\n",
            "checkpoint-5000   custom_null_odds.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnOX9wfQngPf"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/drive/My Drive/final_project')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8Np96eu2eAk",
        "outputId": "d17cfc12-99d4-463a-8e3a-fa5511dc4408"
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/nlpyang/pytorch-transformers/master/examples/utils_squad.py'\n",
        "!wget 'https://raw.githubusercontent.com/nlpyang/pytorch-transformers/master/examples/utils_squad_evaluate.py'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-05 04:47:22--  https://raw.githubusercontent.com/nlpyang/pytorch-transformers/master/examples/utils_squad.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41529 (41K) [text/plain]\n",
            "Saving to: ‘utils_squad.py.6’\n",
            "\n",
            "utils_squad.py.6    100%[===================>]  40.56K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-05-05 04:47:23 (11.2 MB/s) - ‘utils_squad.py.6’ saved [41529/41529]\n",
            "\n",
            "--2021-05-05 04:47:23--  https://raw.githubusercontent.com/nlpyang/pytorch-transformers/master/examples/utils_squad_evaluate.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12493 (12K) [text/plain]\n",
            "Saving to: ‘utils_squad_evaluate.py.6’\n",
            "\n",
            "utils_squad_evaluat 100%[===================>]  12.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-05 04:47:23 (126 MB/s) - ‘utils_squad_evaluate.py.6’ saved [12493/12493]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yxjlwseeykl"
      },
      "source": [
        "from utils_squad import (read_squad_examples, convert_examples_to_features,\n",
        "                         RawResult, write_predictions,\n",
        "                         RawResultExtended, write_predictions_extended)\n",
        "from utils_squad_evaluate import EVAL_OPTS, main as evaluate_on_squad, plot_pr_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGVWI85yfaPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef8e486-dae9-436b-cf60-312d68c03bf8"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHgpndcloLFp"
      },
      "source": [
        "input_file = '/drive/My Drive/final_project/train-v2.0.json'\n",
        "examples = read_squad_examples(input_file=input_file,\n",
        "                                is_training=True,\n",
        "                                version_2_with_negative=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVeg2Gw2mJYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10460019-66fe-40c7-a00f-bc186b605af9"
      },
      "source": [
        "examples[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[qas_id: 56be85543aeaaa14008c9063, question_text: When did Beyonce start becoming popular?, doc_tokens: [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".], start_position: 39, end_position: 42,\n",
              " qas_id: 56be85543aeaaa14008c9065, question_text: What areas did Beyonce compete in when she was growing up?, doc_tokens: [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".], start_position: 28, end_position: 30,\n",
              " qas_id: 56be85543aeaaa14008c9066, question_text: When did Beyonce leave Destiny's Child and become a solo singer?, doc_tokens: [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".], start_position: 82, end_position: 82,\n",
              " qas_id: 56bf6b0f3aeaaa14008c9601, question_text: In what city and state did Beyonce  grow up? , doc_tokens: [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".], start_position: 22, end_position: 23,\n",
              " qas_id: 56bf6b0f3aeaaa14008c9602, question_text: In which decade did Beyonce become famous?, doc_tokens: [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".], start_position: 41, end_position: 42]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWRV-Q10qrIM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "e5b6c8c4-2c89-48c7-b124-cbf02b1686eb"
      },
      "source": [
        "train_data = pd.DataFrame.from_records([vars(example) for example in examples])\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qas_id</th>\n",
              "      <th>question_text</th>\n",
              "      <th>doc_tokens</th>\n",
              "      <th>orig_answer_text</th>\n",
              "      <th>start_position</th>\n",
              "      <th>end_position</th>\n",
              "      <th>is_impossible</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>39</td>\n",
              "      <td>42</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>28</td>\n",
              "      <td>30</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n",
              "      <td>2003</td>\n",
              "      <td>82</td>\n",
              "      <td>82</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>22</td>\n",
              "      <td>23</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>41</td>\n",
              "      <td>42</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     qas_id  ... is_impossible\n",
              "0  56be85543aeaaa14008c9063  ...         False\n",
              "1  56be85543aeaaa14008c9065  ...         False\n",
              "2  56be85543aeaaa14008c9066  ...         False\n",
              "3  56bf6b0f3aeaaa14008c9601  ...         False\n",
              "4  56bf6b0f3aeaaa14008c9602  ...         False\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNVta-8TrNN8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "c9db7f7a-9bee-44a5-e573-697017d1334d"
      },
      "source": [
        "sample = train_data.sample(frac=1).head(1)\n",
        "context = sample.doc_tokens.values\n",
        "train_data[train_data.doc_tokens.values==context]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qas_id</th>\n",
              "      <th>question_text</th>\n",
              "      <th>doc_tokens</th>\n",
              "      <th>orig_answer_text</th>\n",
              "      <th>start_position</th>\n",
              "      <th>end_position</th>\n",
              "      <th>is_impossible</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>79997</th>\n",
              "      <td>5727b298ff5b5019007d92ca</td>\n",
              "      <td>What group was the main antagonist during the ...</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td>the Communists</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79998</th>\n",
              "      <td>5727b298ff5b5019007d92cb</td>\n",
              "      <td>Who led the defense of Chongqing in November 1...</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td>Chiang Kai-Shek</td>\n",
              "      <td>58</td>\n",
              "      <td>59</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79999</th>\n",
              "      <td>5727b298ff5b5019007d92cc</td>\n",
              "      <td>On what date in 1949 did Chengdu fall to the c...</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td>10 December</td>\n",
              "      <td>86</td>\n",
              "      <td>87</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80000</th>\n",
              "      <td>5727b298ff5b5019007d92cd</td>\n",
              "      <td>Why did Sichuan see some communist activity?</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td>it was one area on the road of the Long March</td>\n",
              "      <td>47</td>\n",
              "      <td>57</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80001</th>\n",
              "      <td>5a513846ce860b001aa3fc2f</td>\n",
              "      <td>What resumed after the First Sino-Japanese War?</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80002</th>\n",
              "      <td>5a513846ce860b001aa3fc30</td>\n",
              "      <td>What happened to the cities of west China?</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80003</th>\n",
              "      <td>5a513846ce860b001aa3fc31</td>\n",
              "      <td>What government fled sichuan again?</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80004</th>\n",
              "      <td>5a513846ce860b001aa3fc32</td>\n",
              "      <td>Who flew from Chongqing to Tawian to lead the ...</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80005</th>\n",
              "      <td>5a513846ce860b001aa3fc33</td>\n",
              "      <td>What other city fell following the fall of Che...</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80006</th>\n",
              "      <td>5a68f3838476ee001a58a8f6</td>\n",
              "      <td>What group was the on the defense during the C...</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80007</th>\n",
              "      <td>5a68f3838476ee001a58a8f7</td>\n",
              "      <td>Who led the defense of Burma in November 1949?</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80008</th>\n",
              "      <td>5a68f3838476ee001a58a8f8</td>\n",
              "      <td>On what date in 1949 did Changdu fall to Chian...</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80009</th>\n",
              "      <td>5a68f3838476ee001a58a8f9</td>\n",
              "      <td>Why did Sichuan see some strongholds on the ma...</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80010</th>\n",
              "      <td>5a68f3838476ee001a58a8fa</td>\n",
              "      <td>Which government tried to make Burma its stron...</td>\n",
              "      <td>[The, Second, Sino-Japanese, War, was, soon, f...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         qas_id  ... is_impossible\n",
              "79997  5727b298ff5b5019007d92ca  ...         False\n",
              "79998  5727b298ff5b5019007d92cb  ...         False\n",
              "79999  5727b298ff5b5019007d92cc  ...         False\n",
              "80000  5727b298ff5b5019007d92cd  ...         False\n",
              "80001  5a513846ce860b001aa3fc2f  ...          True\n",
              "80002  5a513846ce860b001aa3fc30  ...          True\n",
              "80003  5a513846ce860b001aa3fc31  ...          True\n",
              "80004  5a513846ce860b001aa3fc32  ...          True\n",
              "80005  5a513846ce860b001aa3fc33  ...          True\n",
              "80006  5a68f3838476ee001a58a8f6  ...          True\n",
              "80007  5a68f3838476ee001a58a8f7  ...          True\n",
              "80008  5a68f3838476ee001a58a8f8  ...          True\n",
              "80009  5a68f3838476ee001a58a8f9  ...          True\n",
              "80010  5a68f3838476ee001a58a8fa  ...          True\n",
              "\n",
              "[14 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX4yzlpmuSkQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86ObEyQtkqjs"
      },
      "source": [
        "import random\n",
        "def print_squad_sample(train_data, line_length=14, separator_length=120):\n",
        "  sample = train_data.sample(frac=1).head(1)\n",
        "  context = sample.doc_tokens.values\n",
        "  print('='*separator_length)\n",
        "  print('CONTEXT: ')\n",
        "  print('='*separator_length)\n",
        "  lines = [' '.join(context[0][idx:idx+line_length]) for idx in range(0, len(context[0]), line_length)]\n",
        "  for l in lines:\n",
        "      print(l)\n",
        "  print('='*separator_length)\n",
        "  questions = train_data[train_data.doc_tokens.values==context]\n",
        "  print('QUESTION:', ' '*(3*separator_length//4), 'ANSWER:')\n",
        "  for idx, row in questions.iterrows():\n",
        "    question = row.question_text\n",
        "    answer = row.orig_answer_text\n",
        "    print(question, ' '*(3*separator_length//4-len(question)+9), (answer if answer else 'No awnser found'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU4S6x0YnnPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc12840b-7e0b-48bb-ff41-de0487bd2501"
      },
      "source": [
        "print_squad_sample(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "CONTEXT: \n",
            "========================================================================================================================\n",
            "The army's major campaign against the Indians was fought in Florida against Seminoles. It\n",
            "took long wars (1818–58) to finally defeat the Seminoles and move them to Oklahoma.\n",
            "The usual strategy in Indian wars was to seize control of the Indians winter\n",
            "food supply, but that was no use in Florida where there was no winter.\n",
            "The second strategy was to form alliances with other Indian tribes, but that too\n",
            "was useless because the Seminoles had destroyed all the other Indians when they entered\n",
            "Florida in the late eighteenth century.\n",
            "========================================================================================================================\n",
            "QUESTION:                                                                                            ANSWER:\n",
            "What Indian tribe was the Army's major campaign against?                                             Seminoles\n",
            "During what years did the wars between the Army and the Seminoles take place?                        1818–58\n",
            "What state were the Seminoles moved to?                                                              Oklahoma\n",
            "What did the Army traditionally take control of to defeat the Indians?                               winter food supply\n",
            "During what century did the Seminoles enter Florida?                                                 eighteenth\n",
            "What American tribe was the Army's major campaign against?                                           No awnser found\n",
            "During what years did the wars between the Army and British take place?                              No awnser found\n",
            "What state did the Pilgrims move to?                                                                 No awnser found\n",
            "What did the Navy traditionally take control of to defeat the Indians?                               No awnser found\n",
            "During what century did the Seminoles enter Texas?                                                   No awnser found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbnphaodvb_u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3yDIGj8t1_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b0cdced9-e0d2-4fa8-a279-8c4dcbf919fd"
      },
      "source": [
        "\n",
        "train_data['paragraph_len'] = train_data['doc_tokens'].apply(len)\n",
        "train_data['question_len'] = train_data['question_text'].apply(len)\n",
        "train_data.sample(frac=1).head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qas_id</th>\n",
              "      <th>question_text</th>\n",
              "      <th>doc_tokens</th>\n",
              "      <th>orig_answer_text</th>\n",
              "      <th>start_position</th>\n",
              "      <th>end_position</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>paragraph_len</th>\n",
              "      <th>question_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>114394</th>\n",
              "      <td>57301295b2c2fd14005687ff</td>\n",
              "      <td>How long did william tubman rule?</td>\n",
              "      <td>[Longstanding, political, tensions, from, the,...</td>\n",
              "      <td>27 year</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>108</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68019</th>\n",
              "      <td>5726b79bdd62a815002e8deb</td>\n",
              "      <td>Where is the North Carolina State Fair?</td>\n",
              "      <td>[The, Time, Warner, Cable, Music, Pavilion, at...</td>\n",
              "      <td>Dorton Arena</td>\n",
              "      <td>101</td>\n",
              "      <td>102</td>\n",
              "      <td>False</td>\n",
              "      <td>131</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5776</th>\n",
              "      <td>56daec56e7c41114004b4b20</td>\n",
              "      <td>Which original judge was a choreographer?</td>\n",
              "      <td>[American, Idol, employs, a, panel, of, judges...</td>\n",
              "      <td>Paula Abdul</td>\n",
              "      <td>27</td>\n",
              "      <td>28</td>\n",
              "      <td>False</td>\n",
              "      <td>85</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113723</th>\n",
              "      <td>5acd8d4907355d001abf46f7</td>\n",
              "      <td>Ice and snow covered prehistoric Ireland and w...</td>\n",
              "      <td>[As, with, most, of, Europe,, prehistoric, Bri...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>196</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115806</th>\n",
              "      <td>5ad4a115ba00c4001a268e6d</td>\n",
              "      <td>How many licenses did Microsoft sell in the fi...</td>\n",
              "      <td>[In, March, 2013,, Microsoft, also, amended, i...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>96</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          qas_id  ... question_len\n",
              "114394  57301295b2c2fd14005687ff  ...           33\n",
              "68019   5726b79bdd62a815002e8deb  ...           39\n",
              "5776    56daec56e7c41114004b4b20  ...           41\n",
              "113723  5acd8d4907355d001abf46f7  ...           64\n",
              "115806  5ad4a115ba00c4001a268e6d  ...           62\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VduHO4WoXwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43328afd-89a6-4955-a5e6-25406577a4bd"
      },
      "source": [
        "max_seq_length = 256\n",
        "print(\"Percentage of context's less than max_seq_length = %s%%\" % (len([l for l in train_data['paragraph_len'] if l <= max_seq_length])/len(train_data) * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of context's less than max_seq_length = 98.19289589392184%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zywEw_BZtx5l"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dj9SoVnuSh2"
      },
      "source": [
        "doc_stride = 128\n",
        "max_seq_length = 256\n",
        "max_query_length = 64\n",
        "# batch size of 64 if RAM available.\n",
        "batch_size = 14"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2oM0q3pdni"
      },
      "source": [
        "cached_features_file = '/drive/My Drive/final_project/cache_train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxXiC4G2uUwa"
      },
      "source": [
        "if not os.path.exists(cached_features_file):\n",
        "  features = convert_examples_to_features(examples=examples,\n",
        "                                        tokenizer=tokenizer,\n",
        "                                        max_seq_length=max_seq_length,\n",
        "                                        doc_stride=doc_stride,\n",
        "                                        max_query_length=max_query_length,\n",
        "                                        is_training=True)\n",
        "  torch.save(features, cached_features_file)\n",
        "else:\n",
        "  features = torch.load(cached_features_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqKRwvvjuW1I"
      },
      "source": [
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8wpGNktunPQ"
      },
      "source": [
        "# Convert to Tensors and build dataset\n",
        "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
        "all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
        "\n",
        "all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
        "all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
        "dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
        "                        all_start_positions, all_end_positions,\n",
        "                        all_cls_index, all_p_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siznTVwRuvkC"
      },
      "source": [
        "train_sampler = RandomSampler(dataset)\n",
        "train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJGpCMcu01s"
      },
      "source": [
        "import glob\n",
        "checkpoints = sorted(glob.glob('/drive/My Drive/final_project/checkpoint*-[0-9]*'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE-mmGhwIJWe"
      },
      "source": [
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d06HFmPmu3Yq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01510bfc-3664-4a00-c9d7-21f52000a197"
      },
      "source": [
        "if len(checkpoints) > 0:\n",
        "  global_step = checkpoints[-1].split('-')[-1]\n",
        "  ckpt_name = '/drive/My Drive/final_project/checkpoint-{}'.format(global_step)\n",
        "  print(\"Loading model from checkpoint %s\" % ckpt_name)\n",
        "  model = BertForQuestionAnswering.from_pretrained(ckpt_name)\n",
        "  train_loss_set_ckpt = torch.load(ckpt_name + '/training_loss.pt')\n",
        "  train_loss_set = to_list(train_loss_set_ckpt)\n",
        "  tr_loss = train_loss_set[-1]\n",
        "else:\n",
        "  global_step = 0\n",
        "  train_loss_set = []\n",
        "  tr_loss = 0.0\n",
        "  model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from checkpoint /drive/My Drive/final_project/checkpoint-9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t19YTyEu5V4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a64fd3-eca3-444a-b9ae-1502ed4ba8d0"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "print(param_optimizer[-2])\n",
        "print(param_optimizer[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('qa_outputs.weight', Parameter containing:\n",
            "tensor([[-1.2649e-02, -6.6199e-03, -3.1451e-03,  ...,  6.7657e-05,\n",
            "         -3.6961e-02,  4.1573e-03],\n",
            "        [-5.7847e-02, -2.8350e-02, -1.7493e-02,  ..., -3.5200e-02,\n",
            "         -2.6129e-03,  1.4186e-02]], device='cuda:0', requires_grad=True))\n",
            "('qa_outputs.bias', Parameter containing:\n",
            "tensor([0.0150, 0.0154], device='cuda:0', requires_grad=True))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9alnEO5Uu7jm"
      },
      "source": [
        "learning_rate = 5e-5\n",
        "adam_epsilon=1e-8\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGwiPd3ju9A1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "0fd80d10-c2ef-456b-fef6-97ff5f290041"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "num_train_epochs = 1\n",
        "\n",
        "print(\"***** Running training *****\")\n",
        "print(\"  Num examples = %d\" % len(dataset))\n",
        "print(\"  Num Epochs = %d\" % num_train_epochs)\n",
        "print(\"  Batch size = %d\" % batch_size)\n",
        "print(\"  Total optimization steps = %d\" % (len(train_dataloader) // num_train_epochs))\n",
        "\n",
        "model.zero_grad()\n",
        "train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
        "set_seed()\n",
        "\n",
        "global_step = int(global_step)\n",
        "for _ in train_iterator:\n",
        "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "      if step < global_step + 1:\n",
        "        continue\n",
        "\n",
        "      model.train()\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "      inputs = {'input_ids':       batch[0],\n",
        "                'attention_mask':  batch[1], \n",
        "                'token_type_ids':  batch[2],  \n",
        "                'start_positions': batch[3], \n",
        "                'end_positions':   batch[4]}\n",
        "\n",
        "      inputs = inputs.to(device)\n",
        "\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "      loss = outputs[0]\n",
        "      train_loss_set.append(loss)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      tr_loss += loss.item()\n",
        "      optimizer.step()\n",
        "      model.zero_grad()\n",
        "      global_step += 1\n",
        "    \n",
        "      if global_step % 1000 == 0:\n",
        "        print(\"Train loss: {}\".format(tr_loss/global_step))\n",
        "        output_dir = '/drive/My Drive/final_project/checkpoint-{}'.format(global_step)\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        print(\"Saving training_loss.pt to %s\" % os.path.join(output_dir, '/training_loss.pt'))\n",
        "        torch.save(torch.tensor(train_loss_set), os.path.join(output_dir, '/training_loss.pt'))\n",
        "        print(\"Saving model checkpoint to %s\" % output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/10304 [00:00<?, ?it/s]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 144262\n",
            "  Num Epochs = 1\n",
            "  Batch size = 14\n",
            "  Total optimization steps = 10304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration:   0%|          | 19/10304 [00:00<01:13, 139.53it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 287/10304 [00:00<00:51, 194.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 574/10304 [00:00<00:35, 270.65it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 911/10304 [00:00<00:25, 373.78it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 1241/10304 [00:00<00:17, 509.23it/s]\u001b[A\n",
            "Iteration:  15%|█▍        | 1516/10304 [00:00<00:13, 673.92it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 1827/10304 [00:00<00:09, 880.92it/s]\u001b[A\n",
            "Iteration:  21%|██        | 2161/10304 [00:00<00:07, 1130.55it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 2450/10304 [00:00<00:05, 1376.46it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 2771/10304 [00:01<00:04, 1660.94it/s]\u001b[A\n",
            "Iteration:  30%|██▉       | 3087/10304 [00:01<00:03, 1936.46it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 3437/10304 [00:01<00:03, 2235.42it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 3764/10304 [00:01<00:02, 2469.29it/s]\u001b[A\n",
            "Iteration:  40%|███▉      | 4084/10304 [00:01<00:02, 2638.10it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 4402/10304 [00:01<00:02, 2751.30it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 4717/10304 [00:01<00:01, 2853.19it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 5050/10304 [00:01<00:01, 2978.50it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 5393/10304 [00:01<00:01, 3100.84it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 5720/10304 [00:01<00:01, 3039.33it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 6055/10304 [00:02<00:01, 3124.74it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 6379/10304 [00:02<00:01, 3156.60it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 6734/10304 [00:02<00:01, 3263.97it/s]\u001b[A\n",
            "Iteration:  69%|██████▊   | 7075/10304 [00:02<00:00, 3305.64it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 7410/10304 [00:02<00:00, 3240.78it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 7746/10304 [00:02<00:00, 3273.60it/s]\u001b[A\n",
            "Iteration:  79%|███████▊  | 8096/10304 [00:02<00:00, 3336.25it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 8432/10304 [00:02<00:00, 3340.74it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 8794/10304 [00:02<00:00, 3418.02it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-54e4e0efbfd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                 'end_positions':   batch[4]}\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUfeq9kO1oYM"
      },
      "source": [
        "output_dir = '/drive/My Drive/final_project/checkpoint-final'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpoznGAgYcWE"
      },
      "source": [
        "train_loss_set_ckpt = torch.load('/drive/My Drive/final_project/checkpoint-final/training_loss.pt')\n",
        "train_loss_set = to_list(train_loss_set_ckpt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z2GxIeN1vqa"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyVaTl1i17jC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b2hzsaq16H6"
      },
      "source": [
        "**Load test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TglKsny312Oh"
      },
      "source": [
        "input_file = '/drive/My Drive/final_project/dev-v2.0.json'\n",
        "val_examples = read_squad_examples(input_file=input_file,\n",
        "                                is_training=False,\n",
        "                                version_2_with_negative=True)\n",
        "doc_stride = 128\n",
        "max_seq_length = 256\n",
        "max_query_length = 64\n",
        "cached_features_file = '/drive/My Drive/final_project/cache_validation'\n",
        "\n",
        "# Cache features for faster loading\n",
        "if not os.path.exists(cached_features_file):\n",
        "  features = convert_examples_to_features(examples=val_examples,\n",
        "                                        tokenizer=tokenizer,\n",
        "                                        max_seq_length=max_seq_length,\n",
        "                                        doc_stride=doc_stride,\n",
        "                                        max_query_length=max_query_length,\n",
        "                                        is_training=False)\n",
        "  torch.save(features, cached_features_file)\n",
        "else:\n",
        "  features = torch.load(cached_features_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLmoahbz1_Hx"
      },
      "source": [
        "# Convert to Tensors and build dataset\n",
        "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
        "all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
        "\n",
        "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
        "                        all_example_index, all_cls_index, all_p_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMMV4KIh2OKt"
      },
      "source": [
        "validation_sampler = SequentialSampler(dataset)\n",
        "validation_dataloader = DataLoader(dataset, sampler=validation_sampler, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLdGjs5G2V5O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbrsNDUm2Url"
      },
      "source": [
        "**Evaluate test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3_CAQUf2asD"
      },
      "source": [
        "\n",
        "def evaluate(model, tokenizer):\n",
        "  print(\"***** Running evaluation *****\")\n",
        "  print(\"  Num examples = %d\" % len(dataset))\n",
        "  print(\"  Batch size = %d\" % batch_size)\n",
        "  all_results = []\n",
        "  predict_file = '/drive/My Drive/final_project/dev-v2.0.json'\n",
        "  for batch in tqdm(validation_dataloader, desc=\"Evaluating\", miniters=100, mininterval=5.0):\n",
        "    model.eval()\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "      inputs = {'input_ids':      batch[0],\n",
        "                'attention_mask': batch[1],\n",
        "                'token_type_ids': batch[2]\n",
        "                }\n",
        "      example_indices = batch[3]\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "    for i, example_index in enumerate(example_indices):\n",
        "      eval_feature = features[example_index.item()]\n",
        "      unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "      result = RawResult(unique_id    = unique_id,\n",
        "                         start_logits = to_list(outputs[0][i]),\n",
        "                         end_logits   = to_list(outputs[1][i]))\n",
        "      all_results.append(result)\n",
        "\n",
        "  # Compute predictions\n",
        "  output_prediction_file = \"/drive/My Drive/final_project/predictions.json\"\n",
        "  output_nbest_file = \"/drive/My Drive/final_project/nbest_predictions.json\"\n",
        "  output_null_log_odds_file = \"/drive/My Drive/final_project/null_odds.json\"\n",
        "  output_dir = \"/drive/My Drive/final_project/predict_results\"\n",
        "\n",
        "  write_predictions(val_examples, features, all_results, 10,\n",
        "                  30, True, output_prediction_file,\n",
        "                  output_nbest_file, output_null_log_odds_file, False,\n",
        "                  True, 0.0)\n",
        "\n",
        "  # Evaluate with the official SQuAD script\n",
        "  evaluate_options = EVAL_OPTS(data_file=predict_file,\n",
        "                               pred_file=output_prediction_file,\n",
        "                               na_prob_file=output_null_log_odds_file,\n",
        "                               out_image_dir=None)\n",
        "  results = evaluate_on_squad(evaluate_options)\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDL42u47WX1L"
      },
      "source": [
        "!touch \"/drive/My Drive/final_project/custom_input.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx4AFpiZWyQ0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROJlB2Np2iTo"
      },
      "source": [
        "results = evaluate(model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4_tXFnX2mge"
      },
      "source": [
        "import json\n",
        "results_json = []\n",
        "for k in enumerate(results.keys()):\n",
        "  result_dict = {k[1] : results[k[1]]}\n",
        "  results_json.append(result_dict)\n",
        "print(results_json)\n",
        "with open('results.json', 'w') as f:\n",
        "  json.dump(results_json, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqL0gh6w-hjC"
      },
      "source": [
        "**Evaluate on any text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M-L4ela-liv"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "def getAnswerToQuestion(question,paragraph,model):\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  encoding = tokenizer.encode_plus(text=question,text_pair=paragraph,add_special=True)\n",
        "\n",
        "  inputs = encoding['input_ids']\n",
        "  sentence_embedding = encoding['token_type_ids']\n",
        "  tokens = tokenizer.convert_ids_to_tokens(inputs)\n",
        "\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "  start_scores, end_scores = model(input_ids=torch.tensor([inputs], device='cuda'), token_type_ids=torch.tensor([sentence_embedding], device='cuda'))\n",
        "\n",
        "\n",
        "\n",
        "  start_index = torch.argmax(start_scores)\n",
        "\n",
        "  end_index = torch.argmax(end_scores)\n",
        "\n",
        "  answer = ' '.join(tokens[start_index:end_index+1])\n",
        "\n",
        "  corrected_answer = ''\n",
        "\n",
        "  for word in answer.split():\n",
        "      \n",
        "      #If it's a subword token\n",
        "      if word[0:2] == '##':\n",
        "          corrected_answer += word[2:]\n",
        "      else:\n",
        "          corrected_answer += ' ' + word\n",
        "\n",
        "  return corrected_answer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LAgGblwxgEC"
      },
      "source": [
        "#If there is no good answer, the bot wont output an answer.\n",
        "\n",
        "question = '''What is Cartography?'''\n",
        "\n",
        "paragraph = '''Cartography studies the representation of the Earth's surface with abstract symbols (map making). Although other subdisciplines of geography rely on maps for presenting their analyses, the actual making of maps is abstract enough to be regarded separately. Cartography has grown from a collection of drafting techniques into an actual science.\n",
        "\n",
        "Cartographers must learn cognitive psychology and ergonomics to understand which symbols convey information about the Earth most effectively, and behavioural psychology to induce the readers of their maps to act on the information. They must learn geodesy and fairly advanced mathematics to understand how the shape of the Earth affects the distortion of map symbols projected onto a flat surface for viewing. It can be said, without much controversy, that cartography is the seed from which the larger field of geography grew. Most geographers will cite a childhood fascination with maps as an early sign they would end up in the field.''' \n",
        "\n",
        "print(getAnswerToQuestion(question,paragraph,model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx6b2a_G0BNi"
      },
      "source": [
        "Code was heavily inspired by the following blog-post: https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b"
      ]
    }
  ]
}